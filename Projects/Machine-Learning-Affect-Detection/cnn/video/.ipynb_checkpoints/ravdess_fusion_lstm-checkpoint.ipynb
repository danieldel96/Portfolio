{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dirty-cradle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functional API fusion model\n",
    "import keras\n",
    "import tensorflow\n",
    "tensorflow.debugging.set_log_device_placement(False)\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "#from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import Conv3D\n",
    "\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.layers import ConvLSTM1D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n",
    "\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import Input\n",
    "\n",
    "from tensorflow.keras import optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import datetime\n",
    "from sklearn.model_selection import KFold\n",
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "574953cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#import tensorflow as tf; print(tf.__version__)\n",
    "#import keras\n",
    "\n",
    "#from keras.layers import ConvLSTM3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d974e71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
     ]
    }
   ],
   "source": [
    "gpus = tensorflow.config.list_physical_devices('GPU')\n",
    "\n",
    "\n",
    "strategy = tensorflow.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\",\"/gpu:2\", \"/gpu:3\"])#devices=[\"/gpu:0\", \"/gpu:1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "amended-malawi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1152  115  128  128]\n",
      "[  1152 110544]\n",
      "[1152]\n"
     ]
    }
   ],
   "source": [
    "load = np.load('data/RAVDESS_basic.npz',allow_pickle=True)\n",
    "audio = load['audio']\n",
    "video = load['video']\n",
    "labels = load['labels']\n",
    "n_splits = 6\n",
    "print(np.array(video.shape))\n",
    "print(np.array(audio.shape))\n",
    "print(np.array(labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "direct-breath",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1152, 23, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "#video pre-process\n",
    "fraction = 5\n",
    "frames = int(video.shape[1]/fraction)\n",
    "shape = (128,128,frames,1)\n",
    "\n",
    "\n",
    "reshape_videos = []\n",
    "\n",
    "for i in video:\n",
    "    single_video = []\n",
    "    for j in range( i.shape[0] ): #HEREi.shape[0]\n",
    "        if(  j % fraction == 0 ):\n",
    "            single_video.append( i[j] )\n",
    "    reshape_videos.append( single_video )\n",
    "\n",
    "video = np.array(reshape_videos)\n",
    "del reshape_videos\n",
    "\n",
    "\n",
    "video_input = (frames, 128,  128, 1)\n",
    "video_shape = (video.shape[0], frames, 128, 128, 1)\n",
    "video = np.reshape(video, video_shape)\n",
    "print(video.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "broadband-financing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1152, 110544, 1)\n"
     ]
    }
   ],
   "source": [
    "#audio pre-process 1dconv\n",
    "num_rows = audio.shape[1]\n",
    "num_columns = 1\n",
    "num_channels = 1\n",
    "\n",
    "audio_input = (num_rows, num_columns)\n",
    "audio_shape = (audio.shape[0], num_rows, num_columns)\n",
    "audio = np.reshape(audio, audio_shape)\n",
    "\n",
    "print(audio.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f381e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1152, 110544, 1)\n"
     ]
    }
   ],
   "source": [
    "#audio pre-process mfcc\n",
    "num_rows = audio.shape[1]\n",
    "num_columns = audio.shape[2]\n",
    "num_channels = 1\n",
    "audio_input = (num_rows, num_columns)\n",
    "audio_shape = (audio.shape[0], num_rows, num_columns)\n",
    "audio = np.reshape(audio, audio_shape)\n",
    "\n",
    "print(audio.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "regional-force",
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = labels - 1\n",
    "labels = to_categorical(labels)\n",
    "target_names = ['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5273791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning fold #0...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer gru_3 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 65536)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-611125550a55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/lib/python3.8/site-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    215\u001b[0m       \u001b[0;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m       \u001b[0;31m# refresh its output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m       \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSINGLE_LAYER_OUTPUT_ERROR_MSG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/lib/python3.8/site-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/lib/python3.8/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    974\u001b[0m     \u001b[0;31m# >> model = tf.keras.Model(inputs, outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0m\u001b[1;32m    977\u001b[0m                                                 input_list)\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/lib/python3.8/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1112\u001b[0m         layer=self, inputs=inputs, build_graph=True, training=training_value):\n\u001b[1;32m   1113\u001b[0m       \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m       outputs = self._keras_tensor_symbolic_call(\n\u001b[0m\u001b[1;32m   1115\u001b[0m           inputs, input_masks, args, kwargs)\n\u001b[1;32m   1116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/lib/python3.8/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    846\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/lib/python3.8/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    884\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m           \u001b[0;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m           \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/lib/python3.8/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2631\u001b[0m     \u001b[0;31m# Check input assumptions set before layer building, e.g. input rank.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2632\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2633\u001b[0;31m       input_spec.assert_input_compatibility(\n\u001b[0m\u001b[1;32m   2634\u001b[0m           self.input_spec, inputs, self.name)\n\u001b[1;32m   2635\u001b[0m       \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/lib/python3.8/site-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    212\u001b[0m       \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\u001b[0m\u001b[1;32m    215\u001b[0m                          \u001b[0mlayer_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' is incompatible with the layer: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                          \u001b[0;34m'expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer gru_3 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 65536)"
     ]
    }
   ],
   "source": [
    "#k-fold testing video\n",
    "n = 0 #current fold\n",
    "\n",
    "epochs = 30\n",
    "name = 'ravdess_' + str(epochs) + '_video'\n",
    "pickle_file = 'pickle_files/' + name + '.pckl'\n",
    "\n",
    "training_score = []\n",
    "testing_score = []\n",
    "\n",
    "f = open(pickle_file, 'wb') #file for saving information/data about each fold\n",
    "\n",
    "\n",
    "kf = KFold(n_splits,shuffle=True,random_state=42)\n",
    "kf.get_n_splits(audio)\n",
    "\n",
    "\n",
    "kf = KFold(n_splits,shuffle=True,random_state=42)\n",
    "kf.get_n_splits(audio)\n",
    "\n",
    "for train_index, test_index in kf.split(audio):\n",
    "    print('Beginning fold #' + str(n) + '...')\n",
    "    #print(train_index)\n",
    "    #print(test_index)\n",
    "    a_train, a_test = audio[train_index], audio[test_index]\n",
    "    v_train, v_test = video[train_index], video[test_index]\n",
    "    l_train, l_test = labels[train_index], labels[test_index]\n",
    "\n",
    "\n",
    "    with strategy.scope():\n",
    "        model = Sequential()\n",
    "        \n",
    "        #model.add(ConvLSTM2D(64, kernel_size=(3,3), strides=(2,2), input_shape=video_input, data_format='channels_last',return_sequences=True))\n",
    "        \n",
    "        model.add(Conv3D(64, kernel_size=(3,3,3), strides=(2,2,1),input_shape=video_input))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        #layer 2\n",
    "        model.add(Conv3D(128, kernel_size=(3,3,3), strides=(2,2,1), padding='SAME'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        #layer 3\n",
    "        model.add(Conv3D(256, kernel_size=(3,3,3), strides=(2,2,1), padding='SAME'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        #layer 4\n",
    "        model.add(Conv3D(512, kernel_size=(3,3,3), strides=(2,2,2), padding='SAME'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        #layer 5\n",
    "        model.add(Conv3D(1024, kernel_size=(3,3,3), strides=(2,2,2), padding='SAME'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        #layer 6\n",
    "        model.add(Conv3D(2048, kernel_size=(3,3,3), strides=(2,2,2), padding='SAME'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        #layer 7\n",
    "        model.add(Flatten())\n",
    "        \n",
    "        model.add(GRU(3))\n",
    "        model.add(Dense(6, activation='softmax'))\n",
    "        \n",
    "                \n",
    "        model.compile(loss='categorical_crossentropy', #categorical_crossentropy\n",
    "                        optimizer=optimizers.Adam(learning_rate=0.00001), \n",
    "                                 metrics=['accuracy'])\n",
    "        model.fit(v_train, l_train,\n",
    "                        batch_size=30, #batch 9\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_split=0.166)\n",
    "        \n",
    "        score = model.evaluate(v_train, l_train, verbose=0)\n",
    "        pickle.dump([score, 'training accuracy fold #' + str(n)], f)\n",
    "        training_score.append(score)\n",
    "\n",
    "        score = model.evaluate(v_test, l_test, verbose=0)\n",
    "        pickle.dump([score, 'testing accuracy fold #' + str(n)], f)\n",
    "        testing_score.append(score)\n",
    "\n",
    "        Y_pred = model.predict(v_test)\n",
    "        y_pred = np.argmax(Y_pred, axis=1)\n",
    "        labels_ = np.argmax(l_test, axis=1)\n",
    "        pickle.dump(confusion_matrix(labels_, y_pred), f)\n",
    "        pickle.dump(classification_report(labels_, y_pred), f)\n",
    "        \n",
    "        n+=1\n",
    "        \n",
    "np.savez('data/'+name,x=training_score,y=testing_score)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09095b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold testing audio\n",
    "n = 0 #current fold\n",
    "\n",
    "epochs = 30\n",
    "name = 'ravdess_' + str(epochs) + '_lstm_audio'\n",
    "pickle_file = 'pickle_files/' + name + '.pckl'\n",
    "\n",
    "training_score = []\n",
    "testing_score = []\n",
    "\n",
    "f = open(pickle_file, 'wb') #file for saving information/data about each fold\n",
    "\n",
    "\n",
    "kf = KFold(n_splits,shuffle=True,random_state=42)\n",
    "kf.get_n_splits(audio)\n",
    "\n",
    "\n",
    "kf = KFold(n_splits,shuffle=True,random_state=42)\n",
    "kf.get_n_splits(audio)\n",
    "\n",
    "for train_index, test_index in kf.split(audio):\n",
    "    print('Beginning fold #' + str(n) + '...')\n",
    "    #print(train_index)\n",
    "    #print(test_index)\n",
    "    a_train, a_test = audio[train_index], audio[test_index]\n",
    "    v_train, v_test = video[train_index], video[test_index]\n",
    "    l_train, l_test = labels[train_index], labels[test_index]\n",
    "\n",
    "\n",
    "    with strategy.scope():\n",
    "        model = Sequential()\n",
    "        model.add(Conv1D(padding='same', filters=25, kernel_size=64, strides = 8, input_shape=audio_input))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Conv1D(padding='same', filters=25, kernel_size=128, strides = 8))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Conv1D(padding='same', filters=25, kernel_size=256, strides = 8))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Conv1D(padding='same', filters=25, kernel_size=512, strides = 4))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Conv1D(padding='same', filters=25, kernel_size=1024, strides = 4))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(GRU(128))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "\n",
    "        model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', #categorical_crossentropy\n",
    "                        optimizer=optimizers.Adam(learning_rate=0.00001), \n",
    "                                 metrics=['accuracy'])\n",
    "        model.fit(a_train, l_train,\n",
    "                        batch_size=30, #batch 9\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_split=0.166)\n",
    "        \n",
    "        score = model.evaluate(a_train, l_train, verbose=0)\n",
    "        pickle.dump([score, 'training accuracy fold #' + str(n)], f)\n",
    "        training_score.append(score)\n",
    "\n",
    "        score = model.evaluate(a_test, l_test, verbose=0)\n",
    "        pickle.dump([score, 'testing accuracy fold #' + str(n)], f)\n",
    "        testing_score.append(score)\n",
    "\n",
    "        Y_pred = model.predict(a_test)\n",
    "        y_pred = np.argmax(Y_pred, axis=1)\n",
    "        labels_ = np.argmax(l_test, axis=1)\n",
    "        pickle.dump(confusion_matrix(labels_, y_pred), f)\n",
    "        pickle.dump(classification_report(labels_, y_pred), f)\n",
    "        \n",
    "        n+=1\n",
    "    break\n",
    "        \n",
    "np.savez('data/'+name,x=training_score,y=testing_score)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34d330a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#k-fold testing video\n",
    "n = 0 #current fold\n",
    "\n",
    "epochs = 30\n",
    "name = 'ravdess_' + str(epochs) + '_lstm_video'\n",
    "pickle_file = 'pickle_files/' + name + '.pckl'\n",
    "\n",
    "training_score = []\n",
    "testing_score = []\n",
    "\n",
    "f = open(pickle_file, 'wb') #file for saving information/data about each fold\n",
    "\n",
    "\n",
    "kf = KFold(n_splits,shuffle=True,random_state=42)\n",
    "kf.get_n_splits(audio)\n",
    "\n",
    "print(kf)\n",
    "\n",
    "print(audio.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "kf = KFold(n_splits,shuffle=True,random_state=42)\n",
    "kf.get_n_splits(audio)\n",
    "\n",
    "for train_index, test_index in kf.split(audio):\n",
    "    print('Beginning fold #' + str(n) + '...')\n",
    "    #print(train_index)\n",
    "    #print(test_index)\n",
    "    a_train, a_test = audio[train_index], audio[test_index]\n",
    "    v_train, v_test = video[train_index], video[test_index]\n",
    "    l_train, l_test = labels[train_index], labels[test_index]\n",
    "\n",
    "    with strategy.scope(): \n",
    "\n",
    "        model = Sequential()\n",
    "       \n",
    "        model.add(ConvLSTM2D(512, kernel_size=(3,3), strides=(2,2), input_shape=video_input, data_format='channels_last',return_sequences=True))\n",
    "        #model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(ConvLSTM2D(256, kernel_size=(3,3), strides=(2,2), data_format='channels_last',return_sequences=True))# padding='SAME'\n",
    "        #model.add(BatchNormalization())\n",
    "        #model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        #model.add(ConvLSTM2D(128, kernel_size=(3,3), strides=(2,2), data_format='channels_last',return_sequences=True))# padding='SAME'\n",
    "        #model.add(BatchNormalization())\n",
    "        #model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        #model.add(ConvLSTM2D(64, kernel_size=(3,3), strides=(2,2), data_format='channels_last',return_sequences=True))# padding='SAME'\n",
    "        #model.add(BatchNormalization())\n",
    "        #model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        #1024 runs out of memory\n",
    "        #model.add(ConvLSTM2D(1024, kernel_size=(3,3), strides=(2,2), data_format='channels_last',return_sequences=True))# padding='SAME'\n",
    "        #model.add(BatchNormalization())\n",
    "        #model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        #model.add(ConvLSTM2D(2048, kernel_size=(3,3), strides=(2,2), data_format='channels_last',return_sequences=True))# padding='SAME'\n",
    "        #model.add(BatchNormalization())\n",
    "        #model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "\n",
    "        model.add(Flatten())\n",
    "\n",
    "        model.add(Dense(6, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', #categorical_crossentropy\n",
    "                        optimizer=optimizers.Adam(learning_rate=0.0001), \n",
    "                                 metrics=['accuracy'])\n",
    "        model.fit(v_train, l_train,\n",
    "                        batch_size=30, #batch 9\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_split=0.166)\n",
    "        \n",
    "        score = model.evaluate(v_train, l_train, verbose=0)\n",
    "        pickle.dump([score, 'training accuracy fold #' + str(n)], f)\n",
    "        training_score.append(score)\n",
    "\n",
    "        score = model.evaluate(v_test, l_test, verbose=0)\n",
    "        pickle.dump([score, 'testing accuracy fold #' + str(n)], f)\n",
    "        testing_score.append(score)\n",
    "\n",
    "        Y_pred = model.predict(v_test)\n",
    "        y_pred = np.argmax(Y_pred, axis=1)\n",
    "        labels_ = np.argmax(l_test, axis=1)\n",
    "        pickle.dump(confusion_matrix(labels_, y_pred), f)\n",
    "        pickle.dump(classification_report(labels_, y_pred), f)\n",
    "        \n",
    "        n+=1\n",
    "        \n",
    "np.savez('data/'+name,x=training_score,y=testing_score)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62a95943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/ravdess_30_video.npz\n",
      "testing_score loss: 0.6772788763046265\n",
      "testing_score accuracy: 0.7612847288449606\n"
     ]
    }
   ],
   "source": [
    "#k_fold testing results\n",
    "print('data/'+name+'.npz')\n",
    "load = np.load('data/'+name+'.npz',allow_pickle=True)\n",
    "\n",
    "training_score = load['x']\n",
    "testing_score = load['y']\n",
    "\n",
    "\n",
    "\n",
    "training_score = np.transpose(training_score)\n",
    "testing_score = np.transpose(testing_score)\n",
    "\n",
    "\n",
    "print('testing_score loss:', np.mean(testing_score[0]))\n",
    "print('testing_score accuracy:',np.mean(testing_score[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850880eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#k_fold testing results\n",
    "print('data/'+name+'.npz')\n",
    "load = np.load('data/'+'ravdess_30_lstm'+'.npz',allow_pickle=True)\n",
    "\n",
    "training_score = load['x']\n",
    "testing_score = load['y']\n",
    "\n",
    "\n",
    "\n",
    "training_score = np.transpose(training_score)\n",
    "testing_score = np.transpose(testing_score)\n",
    "\n",
    "\n",
    "print('testing_score loss:', np.mean(testing_score[0]))\n",
    "print('testing_score accuracy:',np.mean(testing_score[1]))\n",
    "\n",
    "with (open('pickle_files/ravdess_30_lstm.pckl', \"rb\")) as openfile:\n",
    "    while True:\n",
    "        try:\n",
    "            print(pickle.load(openfile))\n",
    "        except EOFError:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f088d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "first layer cnnlstm\n",
    "testing_score loss: 0.6772788763046265\n",
    "testing_score accuracy: 0.7612847288449606"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff46c335",
   "metadata": {},
   "source": [
    "ravdess_30_lstm full lstm architecture\n",
    "testing_score loss: 0.7224234342575073\n",
    "testing_score accuracy: 0.7569444477558136"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a26402",
   "metadata": {},
   "outputs": [],
   "source": [
    "data/ravdess_30_video.npz full cnn architecture\n",
    "testing_score loss: 1.0944909354050953\n",
    "testing_score accuracy: 0.7864583333333334"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
