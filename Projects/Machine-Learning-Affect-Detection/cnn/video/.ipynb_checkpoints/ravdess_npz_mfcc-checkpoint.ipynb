{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f210c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#audio and video to npz file\n",
    "import glob\n",
    "import librosa\n",
    "import numpy as np\n",
    "import ffmpeg\n",
    "import datetime as dt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e39c47b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n",
    "\n",
    "zero = np.full((100,100),100)\n",
    "fig = plt.figure()\n",
    "plt.imshow(zero,cmap='gray',vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65ce081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# npz[*] = [ [video audio] ] = [ [ 384 x 128 x 128 x 16] [ 105211 ] ] \n",
    "\n",
    "\n",
    "path = 'dataset/RAVDESS/Video_Speech_Actor_**/**/*.mp4'\n",
    "#sample video from dataset\n",
    "files = glob.glob(path)\n",
    "probe = ffmpeg.probe(files[0])\n",
    "video_stream = next((stream for stream in probe['streams'] if stream['codec_type'] == 'video'), None)\n",
    "width = int(video_stream['width'])\n",
    "height = int(video_stream['height'])\n",
    "\n",
    "pre_time = dt.datetime.now()\n",
    "\n",
    "audio_set = 110544\n",
    "\n",
    "video_set = 115 #115\n",
    "\n",
    "#******************************\n",
    "label_array = [3,4,5,6,7,8]\n",
    "\n",
    "\n",
    "#*******************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df3092f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#audio function\n",
    "def extract_audio(filename):\n",
    "    audio, sample_rate = librosa.load(filename, res_type = 'kaiser_fast') #audio function\n",
    "    fxn()\n",
    "    size = audio.shape[0]\n",
    "    \n",
    "    odd = size%2\n",
    "    \n",
    "    if( size > audio_set ):\n",
    "        offset = int((size - audio_set) / 2)\n",
    "        start = offset\n",
    "        end = size - offset - odd\n",
    "        #print(size, start, end, end-start)\n",
    "        audio = audio[start:end]\n",
    "        \n",
    "    if( size < audio_set ):\n",
    "        offset = int((audio_set - size) / 2)\n",
    "        start = np.zeros((offset))\n",
    "        end = np.zeros((offset+odd))\n",
    "        audio = np.concatenate((start,audio,end), axis=0)\n",
    "    \n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99344471",
   "metadata": {},
   "outputs": [],
   "source": [
    "#video function\n",
    "def extract_video(filename):\n",
    "    out, _ = (\n",
    "        ffmpeg\n",
    "        .input(filename)\n",
    "        .output('pipe:', format='rawvideo', pix_fmt='rgb24', loglevel='quiet')\n",
    "        .run(capture_stdout=True)\n",
    "    )\n",
    "    #convert video to numpy array\n",
    "    video = (\n",
    "        np\n",
    "        .frombuffer(out, np.uint8)\n",
    "        .reshape([-1, height, width, 3])\n",
    "    )\n",
    "    size = video.shape[0]\n",
    "    \n",
    "    odd = not(size%2)\n",
    "    \n",
    "    if( size > video_set ):\n",
    "        #print('more: ', size)\n",
    "        offset = int((size - video_set) / 2)\n",
    "        \n",
    "        start = offset\n",
    "        \n",
    "        end = size - offset - odd\n",
    "        \n",
    "        #print(size, start, end, end-start)\n",
    "        \n",
    "        video = video[start:end]\n",
    "        \n",
    "    if( size < video_set ):\n",
    "        #print('less: ', size)\n",
    "        offset = int((video_set - size) / 2)\n",
    "        #print('offset', offset)\n",
    "        start = video[0]\n",
    "        start = np.tile(start,(offset,1))\n",
    "        start = start.reshape(offset, height, width, 3)\n",
    "        \n",
    "        end = video[size-1]\n",
    "        end = np.tile(end,(offset + odd,1))\n",
    "        end = end.reshape(offset + odd, height, width, 3)\n",
    "        \n",
    "        #print(start.shape)\n",
    "        #print(video.shape)\n",
    "        #print(end.shape)\n",
    "\n",
    "        video = np.concatenate((start,video,end),axis=0)\n",
    "    \n",
    "    #print(video[0].shape)\n",
    "    #new video array\n",
    "    new = []\n",
    "    \n",
    "    for frame in video: # need to figure out size matching TODO\n",
    "    #print( video[frame].shape )\n",
    "        new_img = cv2.resize(frame, (128,128))#, interpolation=cv2.INTER_AREA)\n",
    "        new_img = cv2.cvtColor(new_img, cv2.COLOR_BGR2GRAY)\n",
    "        new.append(new_img)\n",
    "    video = np.array(new)\n",
    "    #print(video.shape[0])\n",
    "    return video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a19cf93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all():\n",
    "    video_files = []\n",
    "    audio_files = []\n",
    "    labels = []\n",
    "\n",
    "    for filename in files:\n",
    "        class_label = int(filename[54])\n",
    "        repitition = int(filename[63])\n",
    "        if( repitition == 1  and (class_label in label_array)):\n",
    "            audio = extract_audio(filename)\n",
    "            video = extract_video(filename)\n",
    "            \n",
    "            audio_files.append(audio)\n",
    "            \n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            fxn()\n",
    "            \n",
    "            video_files.append(video)\n",
    "            \n",
    "            labels.append(class_label-3)\n",
    "            #print(class_label)\n",
    "            \n",
    "            delta = dt.datetime.now()\n",
    "            if( (int((delta - pre_time).seconds) % 60) == 0 ):\n",
    "                print('audio size: ', np.array(audio).shape )\n",
    "                print('video size: ', np.array(video).shape )\n",
    "                print('label: ', class_label)\n",
    "                #plt.imshow(video[50],cmap='gray')\n",
    "                #plt.show()\n",
    "\n",
    "    return (audio_files, video_files, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "broken-skirt",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio size:  (110544,)\n",
      "video size:  (115, 128, 128)\n",
      "label:  3\n",
      "audio size:  (110544,)\n",
      "video size:  (115, 128, 128)\n",
      "label:  3\n",
      "audio size:  (110544,)\n",
      "video size:  (115, 128, 128)\n",
      "label:  5\n",
      "audio size:  (110544,)\n",
      "video size:  (115, 128, 128)\n",
      "label:  3\n",
      "audio size:  (110544,)\n",
      "video size:  (115, 128, 128)\n",
      "label:  5\n",
      "audio size:  (110544,)\n",
      "video size:  (115, 128, 128)\n",
      "label:  7\n",
      "audio size:  (110544,)\n",
      "video size:  (115, 128, 128)\n",
      "label:  3\n",
      "audio size:  (110544,)\n",
      "video size:  (115, 128, 128)\n",
      "label:  4\n",
      "audio size:  (110544,)\n",
      "video size:  (115, 128, 128)\n",
      "label:  5\n",
      "audio size:  (110544,)\n",
      "video size:  (115, 128, 128)\n",
      "label:  4\n",
      "audio size:  (110544,)\n",
      "video size:  (115, 128, 128)\n",
      "label:  3\n",
      "audio size:  (110544,)\n",
      "video size:  (115, 128, 128)\n",
      "label:  4\n",
      "audio size:  (110544,)\n",
      "video size:  (115, 128, 128)\n",
      "label:  5\n",
      "audio size:  (110544,)\n",
      "video size:  (115, 128, 128)\n",
      "label:  6\n",
      "audio size:  (110544,)\n",
      "video size:  (115, 128, 128)\n",
      "label:  8\n",
      "audio size:  (110544,)\n",
      "video size:  (115, 128, 128)\n",
      "label:  5\n",
      "audio size:  (110544,)\n",
      "video size:  (115, 128, 128)\n",
      "label:  4\n",
      "audio size:  (110544,)\n",
      "video size:  (115, 128, 128)\n",
      "label:  6\n",
      "audio size:  (110544,)\n",
      "video size:  (115, 128, 128)\n",
      "label:  8\n",
      "audio size:  (110544,)\n",
      "video size:  (115, 128, 128)\n",
      "label:  8\n",
      "audio size:  (110544,)\n",
      "video size:  (115, 128, 128)\n",
      "label:  3\n",
      "audio size:  (110544,)\n",
      "video size:  (115, 128, 128)\n",
      "label:  6\n",
      "audio size:  (110544,)\n",
      "video size:  (115, 128, 128)\n",
      "label:  5\n",
      "audio size:  (110544,)\n",
      "video size:  (115, 128, 128)\n",
      "label:  6\n"
     ]
    }
   ],
   "source": [
    "audio, video, labels = extract_all()\n",
    "np.savez('data/RAVDESS_basic.npz', audio=audio, video=video, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50b132d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "load = np.load('data/explore_audio.npz',allow_pickle=True)\n",
    "audio = load['audio']\n",
    "video = load['video']\n",
    "labels = load['labels']\n",
    "print(np.array(audio.shape))\n",
    "plt.hist(audio,120)\n",
    "plt.show()\n",
    "median = np.median(audio)\n",
    "print(median)\n",
    "mean = np.mean(audio)\n",
    "print(mean)\n",
    "\n",
    "print('min', np.amin(audio))\n",
    "print('max', np.amax(audio))\n",
    "\n",
    "stdevs = np.std(audio)\n",
    "plt.hist(stdevs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e121ab6f",
   "metadata": {},
   "source": [
    "idea #1\n",
    "======\n",
    "\n",
    "\n",
    "convert all sizes to mean, easier for arrays bigger, but filling in gaps until ratio reached\n",
    "frames = video.shape[0] # number of frames\n",
    "set_median = median\n",
    "single_median = np.median(frames)\n",
    "ratio = set_median / single_median\n",
    "\n",
    "\n",
    "\n",
    "case ratio < 1\n",
    "    #fill in gaps to reach\n",
    "    new_array = []\n",
    "    index = 0 \n",
    "    while(index < single_median):\n",
    "        index += ratio\n",
    "        new_array.append(video[int(index)])\n",
    "    new_array = new_array[:set_median]\n",
    "    \n",
    "    paulstretch\n",
    "case ratio =>1\n",
    "    samples from frames until ratio reached\n",
    "    new_array = []\n",
    "    index = 0\n",
    "    while(index < single_median)\n",
    "        index += ratio\n",
    "        new_array.append(video[int(index)])\n",
    "    new_array = new_array[:set_median]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04655f69",
   "metadata": {},
   "source": [
    "idea #2 (from paper)\n",
    "================\n",
    "Since emotional video samples may have different duration, we split each of them into a certain number of overlapping segments and then learn audio-visual features from each segment. This also enlarges the amount of training data for our deep models.\n",
    "\n",
    "(from paper)\n",
    "To make sure each video segment has 16 frames, i.e., the input size in C3D-Sports-1M model [33], we delete the first and last (L−16)/2 overlapping frames if a video segment has L≥16 frames. On the contrary, for L<16 we repeat the first and last 16−L2 overlapping frames. It should be noted that, since we employ 64 audio frames in a context window to divide the extracted log Mel-spectrogram into audio segments, the durance of each segment is 655ms corresponding to about 20 video frames in each video segment, i.e., 0.655 s ×30 frame/s. In this case, our implementation does not need to deal with the case with L<16 frames. By contrast, when using 15 audio frames of Mel-spectrogram segments corresponding to about 5 video frames (L=5 ) for experiments, we need to repeat the first 5 and last 6 overlapping frames. We denote the visual input as v .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7905d244",
   "metadata": {},
   "source": [
    "idea #3\n",
    "========\n",
    "Sample from video frames to achieve atleast 16 frames from video, minimal\n",
    "\n",
    "if L<16\n",
    "    fill with first and last reaches 16\n",
    "    \n",
    "if L>16\n",
    "    sample every L/16\n",
    "    until 16 frames achieved\n",
    "    \n",
    "median = 115\n",
    "\n",
    "if L<115\n",
    "    repeat first and last frames until reaches 115\n",
    "if L>115\n",
    "    remove first and last frames until reaches 115"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
