{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2f210c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#audio and video to npz file\n",
    "import glob\n",
    "import librosa\n",
    "import numpy as np\n",
    "import ffmpeg\n",
    "import datetime as dt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "609e7f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n",
    "\n",
    "zero = np.full((100,100),100)\n",
    "fig = plt.figure()\n",
    "plt.imshow(zero,cmap='gray',vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "65ce081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# npz[*] = [ [video audio] ] = [ [ 384 x 128 x 128 x 16] [ 105211 ] ] \n",
    "\n",
    "path = 'dataset/baum/**/*.mp4'\n",
    "\n",
    "#sample video from dataset\n",
    "files = glob.glob(path)\n",
    "probe = ffmpeg.probe(files[0])\n",
    "video_stream = next((stream for stream in probe['streams'] if stream['codec_type'] == 'video'), None)\n",
    "width = int(video_stream['width'])\n",
    "height = int(video_stream['height'])\n",
    "\n",
    "pre_time = dt.datetime.now()\n",
    "\n",
    "audio_set = 84672\n",
    "\n",
    "video_set = 115\n",
    "\n",
    "#******************************\n",
    "#def find_excel(filename):\n",
    "\n",
    "data = pd.read_excel('data/Annotations_BAUM1s.xlsx')\n",
    "\n",
    "\n",
    "#*******************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "df3092f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#audio function\n",
    "def extract_audio(filename):\n",
    "    audio, sample_rate = librosa.load(filename, res_type = 'kaiser_fast') #audio function\n",
    "    fxn()\n",
    "    size = audio.shape[0]\n",
    "    \n",
    "    odd = size%2\n",
    "    \n",
    "    if( size > audio_set ):\n",
    "        offset = int((size - audio_set) / 2)\n",
    "        start = offset\n",
    "        end = size - offset - odd\n",
    "        #print(size, start, end, end-start)\n",
    "        audio = audio[start:end]\n",
    "        \n",
    "    if( size < audio_set ):\n",
    "        offset = int((audio_set - size) / 2)\n",
    "        start = np.zeros((offset))\n",
    "        end = np.zeros((offset+odd))\n",
    "        audio = np.concatenate((start,audio,end), axis=0)\n",
    "    \n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "99344471",
   "metadata": {},
   "outputs": [],
   "source": [
    "#video function\n",
    "def extract_video(filename):\n",
    "    out, _ = (\n",
    "        ffmpeg\n",
    "        .input(filename)\n",
    "        .output('pipe:', format='rawvideo', pix_fmt='rgb24', loglevel='quiet')\n",
    "        .run(capture_stdout=True)\n",
    "    )\n",
    "    #convert video to numpy array\n",
    "    video = (\n",
    "        np\n",
    "        .frombuffer(out, np.uint8)\n",
    "        .reshape([-1, height, width, 3])\n",
    "    )\n",
    "    size = video.shape[0]\n",
    "    \n",
    "    odd = not(size%2)\n",
    "    \n",
    "    if( size > video_set ):\n",
    "        #print('more: ', size)\n",
    "        offset = int((size - video_set) / 2)\n",
    "        \n",
    "        start = offset\n",
    "        \n",
    "        end = size - offset - odd\n",
    "        \n",
    "        #print(size, start, end, end-start)\n",
    "        \n",
    "        video = video[start:end]\n",
    "        \n",
    "    if( size < video_set ):\n",
    "        #print('less: ', size)\n",
    "        offset = int((video_set - size) / 2)\n",
    "        #print('offset', offset)\n",
    "        start = video[0]\n",
    "        start = np.tile(start,(offset,1))\n",
    "        start = start.reshape(offset, 480, 854, 3)\n",
    "        \n",
    "        end = video[size-1]\n",
    "        end = np.tile(end,(offset + odd,1))\n",
    "        end = end.reshape(offset + odd, 480, 854, 3)\n",
    "        \n",
    "        #print(start.shape)\n",
    "        #print(video.shape)\n",
    "        #print(end.shape)\n",
    "\n",
    "        video = np.concatenate((start,video,end),axis=0)\n",
    "    \n",
    "    #print(video[0].shape)\n",
    "    #new video array\n",
    "    new = []\n",
    "    \n",
    "    for frame in video: # need to figure out size matching TODO\n",
    "    #print( video[frame].shape )\n",
    "        new_img = cv2.resize(frame, (128,128))#, interpolation=cv2.INTER_AREA)\n",
    "        new_img = cv2.cvtColor(new_img, cv2.COLOR_BGR2GRAY)\n",
    "        new.append(new_img)\n",
    "    video = np.array(new)\n",
    "    #print(video.shape[0])\n",
    "    return video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a19cf93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all():\n",
    "    video_files = []\n",
    "    audio_files = []\n",
    "    labels = []\n",
    "\n",
    "    for filename in files:\n",
    "        name = filename[18:26]\n",
    "        temp = data[data['Clip Name'] == name].index\n",
    "        class_label = data['Emotion Code'].loc[temp] #gets emotion given location of clip name\n",
    "        print(class_label.item())\n",
    "        \n",
    "        video = extract_video(filename)\n",
    "        audio = extract_audio(filename)\n",
    "        fxn()\n",
    "        \n",
    "        audio_files.append(audio)\n",
    "        video_files.append(video)\n",
    "        #labels.append(class_label)\n",
    "        \n",
    "        \n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        fxn()\n",
    "        \n",
    "        delta = dt.datetime.now()\n",
    "        if( (int((delta - pre_time).seconds) % 60) == 0 ):\n",
    "            print('audio size: ', audio.size )\n",
    "            print('video size: ', video.size )\n",
    "            plt.imshow(video[80])\n",
    "            plt.show()\n",
    "\n",
    "    return (audio_files, video_files, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "broken-skirt",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "can only convert an array of size 1 to a Python scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-165-43940f7ef5d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavez\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/BAUM.npz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-164-2a1c190b4ce0>\u001b[0m in \u001b[0;36mextract_all\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Clip Name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mclass_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Emotion Code'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#gets emotion given location of clip name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mvideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lib/python3.8/site-packages/pandas/core/base.py\u001b[0m in \u001b[0;36mitem\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"can only convert an array of size 1 to a Python scalar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: can only convert an array of size 1 to a Python scalar"
     ]
    }
   ],
   "source": [
    "audio, video, labels = extract_all()\n",
    "np.savez('data/BAUM.npz', audio=audio, video=video, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50b132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load = np.load('data/explore_audio.npz',allow_pickle=True)\n",
    "audio = load['audio']\n",
    "video = load['video']\n",
    "labels = load['labels']\n",
    "print(np.array(audio.shape))\n",
    "plt.hist(audio,120)\n",
    "plt.show()\n",
    "median = np.median(audio)\n",
    "print(median)\n",
    "mean = np.mean(audio)\n",
    "print(mean)\n",
    "\n",
    "print('min', np.amin(audio))\n",
    "print('max', np.amax(audio))\n",
    "\n",
    "stdevs = np.std(audio)\n",
    "plt.hist(stdevs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e121ab6f",
   "metadata": {},
   "source": [
    "idea #1\n",
    "======\n",
    "\n",
    "\n",
    "convert all sizes to mean, easier for arrays bigger, but filling in gaps until ratio reached\n",
    "frames = video.shape[0] # number of frames\n",
    "set_median = median\n",
    "single_median = np.median(frames)\n",
    "ratio = set_median / single_median\n",
    "\n",
    "\n",
    "\n",
    "case ratio < 1\n",
    "    #fill in gaps to reach\n",
    "    new_array = []\n",
    "    index = 0 \n",
    "    while(index < single_median):\n",
    "        index += ratio\n",
    "        new_array.append(video[int(index)])\n",
    "    new_array = new_array[:set_median]\n",
    "    \n",
    "    paulstretch\n",
    "case ratio =>1\n",
    "    samples from frames until ratio reached\n",
    "    new_array = []\n",
    "    index = 0\n",
    "    while(index < single_median)\n",
    "        index += ratio\n",
    "        new_array.append(video[int(index)])\n",
    "    new_array = new_array[:set_median]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04655f69",
   "metadata": {},
   "source": [
    "idea #2 (from paper)\n",
    "================\n",
    "Since emotional video samples may have different duration, we split each of them into a certain number of overlapping segments and then learn audio-visual features from each segment. This also enlarges the amount of training data for our deep models.\n",
    "\n",
    "(from paper)\n",
    "To make sure each video segment has 16 frames, i.e., the input size in C3D-Sports-1M model [33], we delete the first and last (L−16)/2 overlapping frames if a video segment has L≥16 frames. On the contrary, for L<16 we repeat the first and last 16−L2 overlapping frames. It should be noted that, since we employ 64 audio frames in a context window to divide the extracted log Mel-spectrogram into audio segments, the durance of each segment is 655ms corresponding to about 20 video frames in each video segment, i.e., 0.655 s ×30 frame/s. In this case, our implementation does not need to deal with the case with L<16 frames. By contrast, when using 15 audio frames of Mel-spectrogram segments corresponding to about 5 video frames (L=5 ) for experiments, we need to repeat the first 5 and last 6 overlapping frames. We denote the visual input as v .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7905d244",
   "metadata": {},
   "source": [
    "idea #3\n",
    "========\n",
    "Sample from video frames to achieve atleast 16 frames from video, minimal\n",
    "\n",
    "if L<16\n",
    "    fill with first and last reaches 16\n",
    "    \n",
    "if L>16\n",
    "    sample every L/16\n",
    "    until 16 frames achieved\n",
    "    \n",
    "median = 115\n",
    "\n",
    "if L<115\n",
    "    repeat first and last frames until reaches 115\n",
    "if L>115\n",
    "    remove first and last frames until reaches 115"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
